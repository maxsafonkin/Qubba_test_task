# Qubba Test Task

## Description

**Задача:** поисковая выдача. Задача проанализровать страницу поисковой выдачи Wildberries, например. Предложить оптимальный вариант для парсинга 30 страниц каждого запроса (считать что поисковых запросов  будет 100 000). Описать как парсить выдачу в разных географических локациях, например Москва/Санкт-Петербург, Новосибирск.

**Цель:** фиксировать поисковую выдачу товара по разным запросам.

## Parsing Process

Для парсинга страниц было решено исследовать приватное API мобильного приложения Wildberries, так как трафик содержит меньше мусора (аналитика и т.п.).

В ходе исследования был найден эндпоинт `exactmatch/ru/common/v11/search`, позволяющий передавать следующие параметры:

- `dest` - пункт выдачи. Повзоляет специфицировать область поисковой выдачи
- `page_number` - номер страницы для пагинации
- `query` - поисковой запрос

Эндпоинт возвращает структуру, содержащую мета-информацию и список товаров:

```json
{
    "metadata": {
        "catalog_type": "preset",
        "catalog_value": "preset=77002058",
        "normquery": "\u043f\u043b\u0430\u0442\u044c\u0435 \u043b\u0435\u0442\u043d\u0435\u0435",
        "search_result": {},
        "name": "\u043f\u043b\u0430\u0442\u044c\u0435+\u043b\u0435\u0442\u043d\u0435\u0435",
        "rmi": "1",
        "title": "\u043f\u043b\u0430\u0442\u044c\u0435+\u043b\u0435\u0442\u043d\u0435\u0435",
        "rs": 90,
        "qv": "AQEFAQIAASk7KzWwQa2yLAqqTbITNGoxXawmLrUrd7KgoUmonqmwqqEw0KYGKA8oyK4dsl2wp6wQnfCvP7Ctrbaw_qi2L7usYq0Sqhwsmy7OMTKxRyX8pgMuDqvVraWfY6p7M4mtNaw2LJsxbCQ5KNGpTqDUMbesqijGsLGsBK6Gq-UujakfrekrBy2SKAywrSlRre0jfB-drTSoxLDOK6SotyCdrR0xj6XEKcypUCrULF4rHSZqKD4obqo0LhIrSKxFrc-Y-inuLnClny1RqKysWCQHrRStYy1MKhoszSwDnmAu6CuGJDOnTKfOKFSfiKomqbMt0o-qLB6q06iPK2mnqiYYKAs",
        "snippet": "sizes"
    },
    "state": 0,
    "version": 2,
    "payloadVersion": 2,
    "data": {
        "products": [],
        "total": 114001
    }
}
```

## Parsing System

Поскольку поисковых запросов будет 100 000, на каждый по 30 запросов, то предлагается использовать микросервисную архитектуру для сбора и обработки данных, что позволит масштабировать систему для асинхронного сбора данных.

Предлагается использовать **event-driven** архитектуру, состоящую из следующих сервисов:

1. **Update Manager** - на регулярной основе запускает парсинг всех товаров из базы данных
2. **Storage Manager** - хранит результаты парсинга, предоставляет интерфейсы для получения данных
3. **Target Adapter** - обрабатывает результат парсинга
4. **Parser** - собирает данные с маркетплейсов

Для обмена данными между сервисами предлагается использовать брокер `RabbitMQ`, для хранения данных - `NoSQL` базу данных `MongoDB`.

**System Flow:**

1. **Update Manager** создает задачи на обработку определенного запроса
2. **Parser** берет задачу, собирает данные и отправляет результат в очередь с результатами
3. **Target Adapter** обрабатывает результаты задач и отправляет их сервису **Storage Manager**
4. **Storage Manager** сохраняет результаты парсинга в базу данных и предоставляет возможность получить к ним доступ

Благодаря асинхронной микросервисной архитектуре можно поднять неограниченное число парсеров, что позволит кратно ускорить сбор данных. Ограничения: вычислительная мощность сервера и скорость соединения.

## Parsing Script

Скрипт `parse.py` асинхронно собирает данные о товарах с сайта Wildberries по заданному поисковому запросу и сохраняет их в файл `products.jsonl` в формате JSONL (один товар - одна строка).

Зависимости:

- Python3.11
- httpx~=0.28.0
- aiofiles~=24.1.0
- pydantic~=2.10.0

Для запуска скрипта необходимо отредактировать параметры поиска в функции `main()` (поисковый запрос, город, количество страниц).

В результате выполнения скрипта будет создан файл `products.jsonl`, где каждая строка — это JSON-объект с информацией о товаре.

Детали:

- Скрипт использует асинхронный клиент httpx для отправки запросов к Wildberries для ускорения парсинга (парсинг 30 страниц по запросу `платье летнее` занял 2.45 секунды для асинхронного скрипта и 37.49 для синхронного)
- Модели данных описаны с помощью Pydantic
- Для записи в файл используется библиотека aiofiles
